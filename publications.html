<!DOCTYPE html>
<html>

<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-164479100-1"></script>
    <script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-164479100-1');







    </script>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>RAIVN Lab - Publications</title>
    <meta name="description" content="RAIVN Lab -- Publications.">
    <link rel="stylesheet" href="./css/main.css">
    <link rel="stylesheet" href="./css/styles.css">
    <link rel="canonical" href="./publications.html">
    <link rel="shortcut icon" type="image/x-icon" href="./images-raivn/favicon.ico">


</head>


<body>

<div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container-fluid">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
                    data-target="#navbar-collapse-1" aria-expanded="false">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>

            <a class="navbar-brand" href="./index.html" style="color:black">RAIVN Lab @ UW</a>
        </div>
        <div class="collapse navbar-collapse" id="navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                <li><a href="./index.html" class="navbar-item-text">Home</a></li>
                <li><a href="./people.html" class="navbar-item-text">People</a></li>
                <li><a href="./publications.html" class="navbar-item-text navbar-item-on">Publications</a></li>
                <li><a href="./resources.html" class="navbar-item-text">Resources</a></li>
                <li><a href="./news_archive.html" class="navbar-item-text">News Archive</a></li>
                <li><a href="https://sites.google.com/cs.washington.edu/uwcseali" target="_blank" class="navbar-item-text">Internal</a></li>
            </ul>
        </div>
    </div>
</div>

<div class="container-fluid">
    <div class="row">
        <div id="gridid" class="col-sm-12">
            <!-- <h2>Featured</h2>
            <div id="carousel" class="carousel slide" data-ride="carousel" data-interval="5000" data-pause="hover">
                <div class="carousel-inner" id="pub-page-carousel-inner">
                    <div class="item active center-cropped">
                        <a href="https://mitchellnw.github.io/blog/2020/supsup/">
                            <div class="carousel-img-title">Supermasks in Superposition</div>
                            <div class="center-cropped-img" style="background-image: url(./images-raivn/paperpic/supsup.png)"></div>
                        </a>
                    </div>
                    <div class="item center-cropped">
                        <a href="https://prior.allenai.org/projects/interactive-visual-navigation">
                            <div class="carousel-img-title">Pushing it out of the Way:<br/>Interactive Visual Navigation</div>
                            <div class="center-cropped-img" style="background-image: url(./images-raivn/paperpic/interactive-visual-navigation.png)"></div>
                        </a>
                    </div>
                    <div class="item center-cropped">
                        <a href="https://github.com/ehsanik/muscleTorch">
                            <div class="carousel-img-title">What Can You Learn from Your Muscles?<br/>Learning Visual Representation from Human Interactions</div>
                            <div class="center-cropped-img" style="background-image: url(./images-raivn/paperpic/muscleTorch.png)"></div>
                        </a>
                    </div>
                    <div class="item center-cropped">
                        <a href="https://danielgordon10.github.io/papers/vince.html">
                            <div class="carousel-img-title">Watching the World Go By:<br/>Representation Learning from Unlabeled Videos</div>
                            <div class="center-cropped-img" style="background-image: url(./images-raivn/paperpic/vince_cropped.jpg)"></div>
                        </a>
                    </div>
                    <div class="item center-cropped">
                        <a href="https://openreview.net/forum?id=UuchYL8wSZo">
                            <div class="carousel-img-title">Learning Generalizable Visual Representations via Interactive Gameplay</div>
                            <div class="center-cropped-img" style="background-image: url(./images-raivn/paperpic/hide&seek.png)"></div>
                        </a>
                    </div>
                </div>
                <a class="left carousel-control" href="#carousel" role="button" data-slide="prev">
                    <span class="glyphicon glyphicon-chevron-left" aria-hidden="true"></span>
                    <span class="sr-only">Previous</span>
                </a>
                <a class="right carousel-control" href="#carousel" role="button" data-slide="next">
                    <span class="glyphicon glyphicon-chevron-right" aria-hidden="true"></span>
                    <span class="sr-only">Next</span>
                </a>
            </div> -->
            <h2>Preprints</h2>
            <ul>

                <li class="publication"><b>SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality</b><br/>
                    <i>Cheng-Yu Hsieh*, Jieyu Zhang*, Zixian Ma, Aniruddha Kembhavi, Ranjay Krishna</i><br/>
                    <a href="https://arxiv.org/abs/2306.14610">pdf</a> | 
                    <a href="https://github.com/RAIVNLab/sugar-crepe">code</a>
                </li>
                <li class="publication"><b>MIMIC: Masked Image Modeling with Image Correspondences</b><br/>
                    <i>Kalyani Marathe, Mahtab Bigverdi, Nishat Khan, Tuhin Kundu, Aniruddha Kembhavi, Linda G. Shapiro, Ranjay Krishna</i><br/>
                    <a href="https://arxiv.org/pdf/2306.15128.pdf">pdf</a> | 
                    <a href="https://github.com/RAIVNLab/MIMIC">code</a>
                </li>
                <li class="publication"><b>AdANNS: A Framework for Adaptive Semantic Search</b><br/>
                    <i>Aniket Rege, Aditya Kusupati, Sharan Ranjit S, Alan Fan, Qingqing Cao, Sham Kakade, Prateek Jain, Ali Farhadi</i><br/>
                    <a href="https://arxiv.org/abs/2305.19435">pdf</a> | 
                    <a href="https://github.com/RAIVNLab/AdANNS">code</a>
                </li>
                <li class="publication"><b>TIFA: Text-to-Image Faithfulness Evaluation with Question Answering</b><br/>
                    <i>Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, Noah A. Smith</i><br/>
                    <a href="https://arxiv.org/abs/2303.11897">pdf</a> | 
                    <a href="https://tifa-benchmark.github.io/">project page</a>
                </li>
                <li class="publication"><b>DataComp: In search of the next generation of multimodal datasets</b><br/>
                    <i>Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, Ludwig Schmidt.</i><br/>
                    <a href="https://arxiv.org/abs/2304.14108">pdf</a> | 
                    <a href="https://datacomp.ai/">project page</a>
                </li>
                <li class="publication"><b>Stable and low-precision training for large-scale vision-language models</b><br/>
                    <i>Mitchell Wortsman, Tim Dettmers, Luke Zettlemoyer, Ari Morcos, Ali Farhadi, Ludwig Schmidt</i><br/>
                    <a href="https://arxiv.org/abs/2304.13013">pdf</a>
                </li>
                <li class="publication"><b>EQUI-VOCAL: Synthesizing Queries for Compositional Video Events from Limited User Interactions</b><br/>
                    <i>Enhao Zhang, Maureen Daum, Dong He, Brandon Haynes, Ranjay Krishna, Magdalena Balazinska</i><br/>
                    <b>Under Review at VLDB 2023</b><br/>
                    <a href="https://arxiv.org/pdf/2301.00929">pdf</a>
                </li>
                <li class="publication"><b>VOCALExplore: Pay-as-You-Go Video Data Exploration and Model Building</b><br/>
                    <i>Maureen Daum, Enhao Zhang, Dong He, Stephen Mussmann, Brandon Haynes, Ranjay Krishna, Magdalena Balazinska</i><br/>
                    <b>Under Review at VLDB 2023</b><br/>
                    <a href="https://arxiv.org/pdf/2303.04068">pdf</a>
                </li>
                <li class="publication"><b>What does a platypus look like? Generating customized prompts for zero-shot image classification</b><br/>
                    <i>Sarah Pratt, Ian Covert, Rosanne Liu, Ali Farhadi</i><br/>
                    <a href="https://arxiv.org/abs/2209.03320">pdf</a> | 
                    <a href="https://github.com/sarahpratt/CuPL">code</a>
                </li>
            </ul>
            <h2>2023</h2>   
            <ul>
                <li class="publication"><b>Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes</b><br/>
                    <i>Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, Tomas Pfister</i><br/>
                    <b>ACL 2023 (Findings)</b><br/>
                    <a href="https://arxiv.org/pdf/2305.02301">pdf</a>
                </li>
                <li class="publication"><b>CREPE: Can Vision-Language Foundation Models Reason Compositionally?</b><br/>
                    <i>Zixian Ma, Jerry Hong, Mustafa Omer Gul, Mona Gandhi, Irena Gao, Ranjay Krishna</i><br/>
                    <b>CVPR 2023</b><br/>
                    <a href="https://arxiv.org/abs/2212.07796">pdf</a>
                </li>
                <li class="publication"><b>Objaverse: A Universe of Annotated 3D Objects</b><br/>
                    <i>Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, Ali Farhadi</i><br/>
                    <b>CVPR 2023</b><br/>
                    <a href="https://arxiv.org/abs/2212.08051">pdf</a> | 
                    <a href="https://objaverse.allenai.org/">project page</a>
                </li>
                <li class="publication"><b>Phone2Proc: Bringing Robust Robots Into Our Chaotic World</b><br/>
                    <i>Matt Deitke, Rose Hendrix, Luca Weihs, Ali Farhadi, Kiana Ehsani, Aniruddha Kembhavi</i><br/>
                    <b>CVPR 2023</b><br/>
                    <a href="https://arxiv.org/abs/2212.04819">pdf</a> | 
                    <a href="https://allenai.org/project/phone2proc/home">project page</a>
                </li>
                <li class="publication"><b>Moving Forward by Moving Backward: Embedding Action Impact over Action Semantics</b><br/>
                    <i>Kuo-Hao Zeng, Luca Weihs, Roozbeh Mottaghi, Ali Farhadi</i><br/>
                    <b>ICLR 2023</b><br/>
                    <a href="https://arxiv.org/abs/2304.12289">pdf</a> | 
                    <a href="https://prior.allenai.org/projects/action-adaptive-policy">project page</a>
                </li>
                <li class="publication"><b>Impossibly Good Experts and How to Follow Them</b><br/>
                    <i>Aaron Walsman, Muru Zhang, Sanjiban Choudhury, Dieter Fox, Ali Farhadi</i><br/>
                    <b>ICLR 2023</b><br/>
                    <a href="https://openreview.net/forum?id=sciA_xgYofB">pdf</a>
                </li>
                <li class="publication"><b>Neural Radiance Field Codebooks</b><br/>
                    <i>Matthew Wallingford, Aditya Kusupati, Alex Fang, Vivek Ramanujan, Aniruddha Kembhavi, Roozbeh Mottaghi, Ali Farhadi</i><br/>
                    <b>ICLR 2023</b><br/>
                    <a href="https://arxiv.org/abs/2301.04101">pdf</a>
                </li>
                <li class="publication"><b>Editing Models with Task Arithmetic</b><br/>
                    <i>Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, Ali Farhadi</i><br/>
                    <b>ICLR 2023</b><br/>
                    <a href="https://arxiv.org/abs/2212.04089">pdf</a> | 
                    <a href="https://github.com/mlfoundations/task_vectors">code</a>
                </li>   
                <li class="publication"><b>lo-fi: distributed fine-tuning without communication</b><br/>
                    <i>Mitchell Wortsman, Suchin Gururangan, Shen Li, Ali Farhadi, Ludwig Schmidt, Michael Rabbat, Ari S. Morcos</i><br/>
                    <b>TMLR</b><br/>
                    <a href="https://arxiv.org/abs/2210.11948">pdf</a>
                </li>
                </ul>
            <h2>2022</h2>            
            <ul>
                <li class="publication"><b>ELIGN: Expectation Alignment as a Multi-Agent Intrinsic Reward</b><br/>
                    <i>Zixian Ma, Rose Wang, Li Fei-Fei, Michael Bernstein, Ranjay Krishna</i><br/>
                    <b>NeurIPS 2022</b><br/>
                    <a href="https://arxiv.org/abs/2210.04365">pdf</a>
                </li>
                <li class="publication"><b>Patching open-vocabulary models by interpolating weights</b><br/>
                    <i>Gabriel Ilharco*, Mitchell Wortsman*, Samir Yitzhak Gadre*, Shuran Song, Hannaneh Hajishirzi, Simon Kornblith, Ali Farhadi, Ludwig Schmidt</i><br/>
                    <b>NeurIPS 2022</b><br/>
                    <a href="https://arxiv.org/abs/2208.05592">pdf</a> | 
                    <a href="https://github.com/mlfoundations/patching">code</a>
                </li>
                <li class="publication"><b>ProcTHOR: Large-Scale Embodied AI Using Procedural Generation</b><br/>
                    <i>Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs, Jordi Salvador, Kiana Ehsani, Winson Han, Eric Kolve, Ali Farhadi, Aniruddha Kembhavi, Roozbeh Mottaghi</i><br/>
                    <b>NeurIPS 2022</b> | <font style="color:green"><b>Outstanding Paper Award</b></font><br/>
                    <a href="https://arxiv.org/abs/2206.06994">pdf</a> |
                    <a href=" https://procthor.allenai.org">project page</a>
                </li>
                <li class="publication"><b>Matryoshka Representation Learning</b><br/>
                    <i>Aditya Kusupati*, Gantavya Bhatt*, Aniket Rege*, Matthew Wallingford, Aditya Sinha, Vivek Ramanujan, William Howard-Snyder, Kaifeng Chen, Sham Kakade, Prateek Jain, Ali Farhadi</i><br/>
                    <b>NeurIPS 2022</b><br/>
                    <a href="https://arxiv.org/abs/2205.13147">pdf</a> |
                    <a href="https://github.com/RAIVNLab/MRL">code</a>
                </li>
                <li class="publication"><b>LAION-5B: An open large-scale dataset for training next generation image-text models</b><br/>
                    <i>Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, Jenia Jitsev</i><br/>
                    <b>NeurIPS 2022 Datasets and Benchmarks track</b> | <font style="color:green"><b>Outstanding Paper Award</b></font><br/>
                    <a href="https://arxiv.org/abs/2210.08402">pdf</a> |
                    <a href="https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/">project page</a>
                </li>
                <li class="publication"><b>Socially situated artificial intelligence enables learning from human interaction</b><br/>
                    <i>Ranjay Krishna, Donsuk Lee, Li Fei-Fei*, Michael Bernstein</i><br/>
                    <b>PNAS 2022</b><br/>
                    <a href="https://www.pnas.org/doi/10.1073/pnas.2115730119">pdf</a>
                </li>
                <li class="publication"><b>Break and Make: Interactive Structural Understanding Using LEGO Bricks</b><br/>
                    <i>Aaron Walsman, Muru Zhang, Klemen Kotar, Karthik Desingh, Ali Farhadi, Dieter Fox</i><br/>
                    <b>ECCV 2022</b><br/>
                    <a href="https://arxiv.org/abs/2207.13738">pdf</a> |
                    <a href="https://github.com/aaronwalsman/ltron">code</a>
                </li>
                <li class="publication"><b>Object Manipulation via Visual Target Localization</b><br/>
                    <i>Kiana Ehsani, Ali Farhadi, Aniruddha Kembhavi, Roozbeh Mottaghi</i><br/>
                    <b>ECCV 2022</b><br/>
                    <a href="https://arxiv.org/abs/2203.08141">pdf</a>
                </li>
                <li class="publication"><b>Exposing the Limits of Video-Text Models through Contrast Sets</b><br/>
                    <i>Jae Sung Park, Sheng Shen, Ali Farhadi, Trevor Darrell, Yejin Choi, Anna Rohrbach</i><br/>
                    <b>NAACL 2022</b><br/>
                    <a href="https://aclanthology.org/2022.naacl-main.261/">pdf</a>
                </li>
                <li class="publication"><b>Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time</b><br/>
                    <i>Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon*, Simon Kornblith*, Ludwig Schmidt*</i><br/>
                    <b>ICML 2022</b><br/>
                    <a href="https://arxiv.org/abs/2203.05482">pdf</a> |
                    <a href="https://github.com/mlfoundations/model-soups">code</a>
                </li>
                <li class="publication"><b>Robust fine-tuning of zero-shot models</b><br/>
                    <i>Mitchell Wortsman*, Gabriel Ilharco*, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, Ludwig Schmidt</i><br/>
                    <b>CVPR 2022</b> | <font style="color:green"><b>Best Paper Honorable Mention</b></font><br/>
                    <a href="https://arxiv.org/abs/2109.01903">pdf</a> | 
                    <a href="https://github.com/mlfoundations/wise-ft">code</a>
                </li>
                <li class="publication"><b>MERLOT Reserve: Neural Script Knowledge through Vision and Language and Sound</b><br/>
                    <i>Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza Salehi, Aditya Kusupati, Jack Hessel, Ali Farhadi, Yejin Choi</i><br/>
                    <b>CVPR 2022</b><br/>
                    <a href="https://arxiv.org/abs/2201.02639">pdf</a> |
                    <a href="https://rowanzellers.com/merlotreserve">project page</a>
                </li>
                <li class="publication"><b>Forward Compatible Training for Representation Learning</b><br/>
                    <i>Vivek Ramanujan, Pavan Kumar Anasosalu Vasu, Ali Farhadi, Oncel Tuzel, Hadi Pouransari</i><br/>
                    <b>CVPR 2022</b><br/>
                    <a href="https://arxiv.org/abs/2112.02805">pdf</a>
                </li>
                <li class="publication"><b>FMeasuring Compositional Consistency for Video Question Answering</b><br/>
                    <i>Mona Gandhi*, Mustafa Omer Gul*, Eva Prakash, Madeleine Grunde-McLaughlin, Ranjay Krishna, Maneesh Agrawala</i><br/>
                    <b>CVPR 2022</b><br/>
                    <a href="https://arxiv.org/abs/2204.07190">pdf</a>
                </li>
                <li class="publication"><b>VOCAL: Video Organization and Interactive AnaLytics</b><br/>
                    <i>Maureem Daum*, Enhao Zhang*, Dong He, Magdalena Balazinska, Brandon Hayes, Ranjay Krishna, Apryle Craig, Aaron Wirsing</i><br/>
                    <b>CIDR 2022</b><br/>
                    <a href="http://cidrdb.org/cidr2022/papers/p41-daum.pdf">pdf</a>
                </li>
                <li class="publication"><b>ProtoSound: A Personalized and Scalable Sound Recognition System for Deaf and Hard-of-Hearing Users</b><br/>
                    <i>Dhruv Jain, Khoa Huynh Anh Nguyen, Steven Goodman, Rachel Grossman-Kahn, Hung Ngo, Aditya Kusupati, Ruofei Du, Alex Olwal, Leah Findlater, Jon E. Froehlich</i><br/>
                    <b>CHI 2022</b><br/>
                    <a href="https://arxiv.org/abs/2202.11134">pdf</a>
                </li>
                <li class="publication"><b>The Introspective Agent: Interdependence of Strategy, Physiology, and Sensing for Embodied Agents</b><br/>
                    <i>Sarah Pratt, Luca Weihs, Ali Farhadi</i><br/>
                    <a href="https://arxiv.org/abs/2201.00411">pdf</a> |
                    <a href="https://github.com/sarahpratt/introspective">code</a>
                </li>
                <li class="publication"><b>LCS: Learning Compressible Subspaces for Adaptive Network Compression at Inference Time</b><br/>
                    <i>Elvis Nunez*, Maxwell Horton*, Anish Prabhu, Anurag Ranjan, Ali Farhadi, Mohammad Rastegari</i><br/>
                    <a href="https://arxiv.org/abs/2110.04252">pdf</a>
                </li>
            </ul>
            <h2>2021</h2>
            <ul>
                <li class="publication"><b>FiG-NeRF: Figure-Ground Neural Radiance Fields for 3D Object Category Modelling</b><br/>
                    <i>Christopher Xie, Keunhong Park, Ricardo Martin-Brualla, Matthew Brown</i><br/>
                    <b>3DV 2021</b><br/>
                    <a href="https://arxiv.org/abs/2104.08418">pdf</a> |
                    <a href="https://fig-nerf.github.io/">project page</a>
                </li>
                <li class="publication"><b>MERLOT: Multimodal Neural Script Knowledge Models</b><br/>
                    <i>Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, Yejin Choi</i><br/>
                    <b>NeurIPS 2021</b><br/>
                    <a href="https://arxiv.org/abs/2106.02636">pdf</a> |
                    <a href="https://rowanzellers.com/merlot/">project page</a>
                </li>
                <li class="publication"><b>LLC: Accurate, Multi-purpose Learnt Low-dimensional Binary Codes</b><br/>
                    <i>Aditya Kusupati, Matthew Wallingford, Vivek Ramanujan, Raghav Somani, Jae Sung Park, Krishna Pillutla, Prateek Jain, Sham Kakade, Ali Farhadi</i><br/>
                    <b>NeurIPS 2021</b><br/>
                    <a href="https://arxiv.org/abs/2106.01487">pdf</a> |
                    <a href="https://github.com/RAIVNLab/LLC">code</a>
                </li>
                <li class="publication"><b>LanguageRefer: Spatial-Language Model for 3D Visual Grounding</b><br/>
                    <i>Junha Roh, Karthik Desingh, Ali Farhadi, Dieter Fox</i><br/>
                    <b>CoRL 2021</b><br/>
                    <a href="https://arxiv.org/abs/2107.03438">pdf</a> |
                    <a href="https://sites.google.com/view/language-refer">project page</a>
                </li>
                <li class="publication"><b>HyperNeRF: A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields</b><br/>
                    <i>	Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-Brualla, Steven M. Seitz</i><br/>
                    <b>SIGGRAPH Asia 2021</b><br/>
                    <a href="https://arxiv.org/abs/2106.13228">pdf</a> |
                    <a href="https://hypernerf.github.io/">project page</a>
                </li>
                <li class="publication"><b>Finetuning Pretrained Transformers into RNNs</b><br/>
                    <i>	Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, Noah A Smith</i><br/>
                    <b>EMNLP 2021</b><br/>
                    <a href="https://arxiv.org/abs/2103.13076">pdf</a>
                </li>
                <li class="publication"><b>Parameter Norm Growth During Training of Transformers</b><br/>
                    <i>	William Merrill, Vivek Ramanujan, Yoav Goldberg, Roy Schwartz, Noah A. Smith</i><br/>
                    <b>EMNLP 2021</b><br/>
                    <a href="https://arxiv.org/abs/2010.09697">pdf</a>
                </li>
                <li class="publication"><b>Deformable Neural Radiance Fields</b><br/>
                    <i>Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Steven M. Seitz, Ricardo Martin-Brualla</i><br/>
                    <b>ICCV 2021</b><br/>
                    <a href="https://arxiv.org/abs/2011.12948">pdf</a> |
                    <a href="https://nerfies.github.io/">project page</a>
                </li>
                <li class="publication"><b>Contrasting Contrastive Self-Supervised Representation Learning Models</b><br/>
                    <i>	Klemen Kotar, Gabriel Ilharco, Ludwig Schmidt, Kiana Ehsani, Roozbeh Mottaghi</i><br/>
                    <b>ICCV 2021</b><br/>
                    <a href="https://arxiv.org/abs/2103.14005">pdf</a>
                </li>
                <li class="publication"><b>PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D World</b><br/>
                    <i>	Rowan Zellers, Ari Holtzman, Matthew Peters, Roozbeh Mottaghi, Aniruddha Kembhavi, Ali Farhadi, Yejin Choi</i><br/>
                    <b>ACL 2021</b><br/>
                    <a href="https://arxiv.org/abs/2106.00188">pdf</a>  |
                    <a href="https://rowanzellers.com/piglet/">project page</a>
                </li>
                <li class="publication"><b>Learning Neural Network Subspaces</b><br/>
                    <i>	Mitchell Wortsman, Maxwell Horton, Carlos Guestrin, Ali Farhadi, Mohammad Rastegari</i><br/>
                    <b>ICML 2021</b><br/>
                    <a href="https://arxiv.org/abs/2102.10472">pdf</a>  |
                    <a href="https://github.com/apple/learning-subspaces">code</a>
                </li>

                <li class="publication"><b>Probing Text Models for Common Ground with Visual Representations</b><br/>
                    <i>Gabriel Ilharco, Rowan Zellers, Ali Farhadi, Hannaneh Hajishirzi</i><br/>
                    <b>NAACL 2021</b><br/>
                    <a href="https://arxiv.org/abs/2005.00619">pdf</a>
                </li>

                <li class="publication"><b>TuringAdvice: A Generative and Dynamic Evaluation of Language Use</b><br/>
                    <i>Rowan Zellers, Ari Holtzman, Elizabeth Clark, Lianhui Qin, Ali Farhadi, Yejin Choi</i><br/>
                    <b>NAACL 2021</b><br/>
                    <a href="https://arxiv.org/abs/2004.03607">pdf</a> |
                    <a href="https://rowanzellers.com/advice/">project page</a>
                </li>

                <li class="publication"><b>Pushing it out of the Way: Interactive Visual Navigation</b><br/>
                    <i>Kuo-Hao Zeng, Luca Weihs, Ali Farhadi, Roozbeh Mottaghi</i><br/>
                    <b>CVPR 2021</b><br/>
                    <a href="https://arxiv.org/abs/2104.14040">pdf</a> |
                    <a href="https://prior.allenai.org/projects/interactive-visual-navigation">project page</a> |
                    <a href="https://github.com/KuoHaoZeng/Interactive_Visual_Navigation">code</a> |
                    <a href="https://youtu.be/q8xqxgnLEY4">video</a>
                </li>

                <li class="publication"><b>ManipulaTHOR: A Framework for Visual Object Manipulation</b><br/>
                    <i>Kiana Ehsani, Winson Han, Alvaro Herrasti, Eli VanderBilt, Eric Kolve, Luca Weihs, Aniruddha Kembhavi, Roozbeh Mottaghi</i><br/>
                    <b>CVPR 2021</b><br/>
                    <a href="https://arxiv.org/abs/2104.11213">pdf</a>  |
                    <a href="https://github.com/allenai/manipulathor">code</a>
                </li>

                <li class="publication"><b>What Can You Learn from Your Muscles? Learning Visual Representation from Human Interactions</b><br/>
                    <i>Kiana Ehsani, Daniel Gordon, Thomas Nguyen, Roozbeh Mottaghi, Ali Farhadi</i><br/>
                    <b>ICLR 2021</b><br/>
                    <a href="https://arxiv.org/abs/2010.08539">pdf</a> |
                    <a href="https://github.com/ehsanik/muscleTorch">code</a>
                </li>
                <li class="publication"><b>Learning Generalizable Visual Representations via Interactive Gameplay</b><br/>
                    <i>Luca Weihs, Aniruddha Kembhavi, Kiana Ehsani, Sarah Pratt, Winson Han, Alvaro Herrasti, Eric Kolve, Dustin Schwenk, Roozbeh Mottaghi, Ali Farhadi</i><br/>
                    <b>ICLR 2021</b><br/>
                    <a href="https://openreview.net/forum?id=UuchYL8wSZo">pdf</a>
                </li>
                <li class="publication"><b>MultiModalQA: complex question answering over text, tables and images</b><br/>
                    <i>Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari Asai, Gabriel Ilharco, Hannaneh Hajishirzi, Jonathan Berant</i><br/>
                    <b>ICLR 2021</b><br/>
                    <a href="https://openreview.net/forum?id=ee6W5UgQLa">pdf</a>
                </li>
                <li class="publication"><b>Layer-Wise Data-Free CNN Compression</b><br/>
                    <i>	Maxwell Horton, Yanzi Jin, Ali Farhadi, Mohammad Rastegari</i><br/>
                    <a href="https://arxiv.org/abs/2011.09058">pdf</a>
                </li>
                <li class="publication"><b>AllenAct: A Framework for Embodied AI Research</b><br/>
                    <i>Luca Weihs, Jordi Salvador, Klemen Kotar, Unnat Jain, Kuo-Hao Zeng, Roozbeh Mottaghi, Aniruddha Kembhavi</i>
                    <br/>
                    <a href="https://arxiv.org/abs/2008.12760">pdf</a> |
                    <a href="https://github.com/allenai/allenact">code</a> |
                    <a href="https://allenact.org/">project page</a>
                </li>
                <li class="publication"><b>Are We Overfitting to Experimental Setups in Recognition?</b><br/>
                    <i>Matthew Wallingford, Aditya Kusupati*, Keivan Alizadeh-Vahid*, Aaron Walsman, Aniruddha Kembhavi, Ali Farhadi</i>
                    <br/>
                    <a href="https://arxiv.org/abs/2007.02519">pdf</a> |
                    <a href="https://github.com/RAIVNLab/InTheWild">code</a> |
                    <a href="https://raivn.cs.washington.edu/projects/FLUID">project page</a>
                </li>
                <li class="publication"><b>Watching the World Go By: Representation Learning from Unlabeled
                    Videos</b><br/>
                    <i>Daniel Gordon, Kiana Ehsani, Dieter Fox, Ali Farhadi</i><br/>
                    <a href="https://arxiv.org/abs/2003.07990">pdf</a> |
                    <a href="https://danielgordon10.github.io/papers/vince.html">project page</a> |
                    <a href="https://github.com/danielgordon10/vince">code</a>
                </li>

                <li class="publication"><b>Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping</b><br/>
                    <i>Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, Noah
                        Smith</i><br/>
                    <a href="https://arxiv.org/abs/2002.06305">pdf</a>
                </li>

            </ul>
            <h2>2020</h2>
            <ul>
                <li class="publication"><b>Natural Language Rationales with Full-Stack Visual Reasoning: From Pixels to Semantic Frames to Commonsense Graphs</b><br/>
                    <i>Ana Marasovic, Chandra Bhagavatula, Jae Sung Park, Ronan Le Bras, Noah A. Smith, Yejin Choi</i><br/>
                    <b>EMNLP (Findings) 2020</b><br/>
                    <a href="https://arxiv.org/abs/2010.07526">pdf</a>
                </li>
                <li class="publication"><b>Multiple Topologies Prediction for Navigation at Unsignalized Intersections</b><br/>
                    <i>Junha Roh*, Christoforos Mavrogiannis*, Rishabh Madan*, Dieter Fox, Siddhartha S. Srinivasa</i><br/>
                    <b>CoRL 2020</b><br/>
                    <a href="https://arxiv.org/abs/2011.03894">pdf</a> |
                    <a href="https://sites.google.com/view/multiple-topologies-prediction">project page</a> |
                    <a href="https://github.com/rohjunha/multiple-topologies-prediction">code</a>
                </li>
                <li class="publication"><b>Supermasks in Superposition</b><br/>
                    <i>Mitchell Wortsman*, Vivek Ramanujan*, Rosanne Liu, Aniruddha Kembhavi, Mohammad Rastegari, Jason Yosinski, Ali Farhadi</i><br/>
                    <b>NeurIPS 2020</b><br/>
                    <a href="https://arxiv.org/abs/2006.14769">pdf</a> |
                    <a href="https://github.com/RAIVNLab/supsup">code</a> |
                    <a href="https://mitchellnw.github.io/blog/2020/supsup">blog</a>
                </li>
                <li class="publication"><b>RNNPool: Efficient Non-linear Pooling for RAM Constrained Inference</b><br/>
                    <i>Oindrila Saha, Aditya Kusupati, Harsha Vardhan Simhadri, Manik Varma, Prateek Jain</i><br/>
                    <b>NeurIPS 2020</b><br/>
                    <a href="https://arxiv.org/abs/2002.11921">pdf</a> |
                    <a href="https://github.com/microsoft/EdgeML">code</a>
                </li>

                <li class="publication"><b>A Cordial Sync: Going Beyond Marginal Policies for Multi-Agent Embodied Tasks</b><br/>
                    <i>Unnat Jain, Luca Weihs, Eric Kolve, Ali Farhadi, Svetlana Lazebnik, Aniruddha Kembhavi, Alexander Schwing</i><br/>
                    <b>ECCV 2020</b><br/>
                    <a href="https://arxiv.org/abs/2007.04979">pdf</a> |
                    <a href="https://unnat.github.io/cordial-sync">project page</a>
                </li>
                <li class="publication"><b>Visual Commonsense Graphs: Reasoning about the Dynamic Context of a Still Image</b><br/>
                    <i>Jae Sung Park, Chandra Bhagavatula, Roozbeh Mottaghi, Ali Farhadi, Yejin Choi</i><br/>
                    <b>ECCV 2020</b><br/>
                    <a href="https://arxiv.org/abs/2004.10796">pdf</a> |
                    <a href="https://visualcomet.xyz">project page</a>
                </li>
                <li class="publication"><b>Grounded Situation Recognition</b><br/>
                    <i>Sarah Pratt, Mark Yatskar, Luca Weihs, Ali Farhadi, Aniruddha Kembhavi</i><br/>
                    <b>ECCV 2020</b><br/>
                    <a href="https://arxiv.org/abs/2003.12058">pdf</a> |
                    <a href="https://prior.allenai.org/projects/gsr">project page</a>
                </li>
                <li class="publication"><b>Identity Aware Multi-Sentence Video Description</b><br/>
                    <i>Jae Sung Park, Trevor Darrell, Anna Rohrbach</i><br/>
                    <b>ECCV 2020</b><br/>
                    <a href="https://arxiv.org/abs/2008.09791">pdf</a> |
                    <a href="https://sites.google.com/site/describingmovies/lsmdc-2019">project page</a>
                </li>

                <li class="publication"><b>Soft Threshold Weight Reparameterization for Learnable Sparsity</b><br/>
                    <i>Aditya Kusupati, Vivek Ramanujan*, Raghav Somani*, Mitchell Wortsman*, Prateek Jain, Sham Kakade, Ali Farhadi</i><br/>
                    <b>ICML 2020</b><br/>
                    <a href="https://arxiv.org/abs/2002.03231">pdf</a> |
                    <a href="https://github.com/RAIVNLab/STR">code</a> |
                    <a href="https://homes.cs.washington.edu/~kusupati/#Kusupati20">project page</a>
                </li>

                <li class="publication"><b>Adversarial Filters of Dataset Biases</b><br/>
                    <i>Ronan Le Bras, Swabha Swayamdipta, Chandra Bhagavatula, Rowan Zellers, Matthew Peters, Ashish Sabharwal, Yejin Choi</i><br/>
                    <b>ICML 2020</b><br/>
                    <a href="https://arxiv.org/abs/2002.04108">pdf</a>
                </li>

                <li class="publication"><b>What’s Hidden in a Randomly Weighted Neural Network?</b><br/>
                    <i>Vivek Ramanujan*, Mitchell Wortsman*, Aniruddha Kembhavi, Ali Farhadi, Mohammad
                        Rastegari </i><br/>
                    <b>CVPR 2020</b><br/>
                    <a href="https://arxiv.org/abs/1911.13299">pdf</a> |
                    <a href="https://github.com/allenai/hidden-networks">code</a>
                </li>

                <li class="publication"><b>RoboTHOR: An Open Simulation-to-Real Embodied AI Platform</b><br/>
                    <i>Matt Deitke, Winson Han, Alvaro Herrasti, Aniruddha Kembhavi, Eric Kolve, Roozbeh
                        Mottaghi, Jordi Salvador, Dustin Schwenk, Eli VanderBilt, Mathew Walingford, Luca Weihs,
                        Mark Yatskar, Ali Farhadi</i> <br/>
                    <b>CVPR 2020</b></i><br/>
                    <a href="https://arxiv.org/abs/2004.06799">pdf</a> |
                    <a href="https://ai2thor.allenai.org/robothor/">project page</a>
                </li>

                <li class="publication"><b>Visual Reaction: Learning to Play Catch with Your Drone</b><br/>
                    <i>Kuo-Hao Zeng, Roozbeh Mottaghi, Luca Weihs, Ali Farhadi </i> <br/>
                    <b>CVPR 2020</b><br/>
                    <a href="https://arxiv.org/abs/1912.02155">pdf</a> |
                    <a href="http://github.com/KuoHaoZeng/Visual_Reaction">code</a>
                </li>

                <li class="publication"><b>Butterfly Transform: An Efficient FFT Based Neural Architecture
                    Design</b><br/>
                    <i>Keivan Alizadeh-Vahid, Anish Prabhu, Ali Farhadi, Mohammad Rastegari</i><br/>
                    <b>CVPR 2020</b><br/>
                    <a href="https://arxiv.org/abs/1906.02256">pdf</a> |
                    <a href="https://github.com/keivanalizadeh/ButterflyTransform">code</a>
                </li>

                <li class="publication"><b>Use the Force, Luke! Learning to Predict Physical Forces by Simulating
                    Effects</b><br/>
                    <i>Kiana Ehsani, Shubham Tulsiani, Saurabh Gupta, Ali Farhadi, Abhinav Gupta</i><br/>
                    <b>CVPR 2020</b><br/>
                    <a href="https://arxiv.org/abs/2003.12045">pdf</a> |
                    <a href="https://ehsanik.github.io/forcecvpr2020/">project page</a> |
                    <a href="https://github.com/ehsanik/touchTorch">code</a>
                </li>

                <li class="publication"><b>ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks</b><br/>
                    <i>Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, Dieter Fox</i><br/>
                    <b>CVPR 2020</b><br/>
                    <a href="https://arxiv.org/abs/1912.01734">pdf</a> |
                    <a href="https://askforalfred.com/">project page</a> |
                    <a href="https://github.com/askforalfred/alfred">code</a>
                </li>
                <li class="publication"><b>LatentFusion: End-to-End Differentiable Reconstruction and Rendering for Unseen Object Pose Estimation</b><br/>
                    <i>Keunhong Park, Arsalan Mousavian, Yu Xiang, Dieter Fox</i><br/>
                    <b>CVPR 2020</b><br/>
                    <a href="https://arxiv.org/abs/1912.00416">pdf</a> |
                    <a href="https://github.com/NVlabs/latentfusion">code</a>
                </li>
            </ul>
            <h2>2019</h2>
            <ul>
                <li class="publication"><b>Discovering Neural Wirings</b><br/>
                    <i>Mitchel Wortsman, Ali Farhadi, Mohammad Rastegari</i><br/>
                    <b>NeurIPS 2019</b><br/>
                    <a href="https://arxiv.org/abs/1906.00586">pdf</a> |
                    <a href="https://prior.allenai.org/projects/discovering-neural-wirings">project page</a> |
                    <a href="https://github.com/allenai/dnw">code</a> |
                    <a href="https://mitchellnw.github.io/blog/2019/dnw/">blog</a>
                </li>
                <li class="publication"><b>Defending Against Neural Fake news</b><br/>
                    <i>Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, Yejin Choi</i><br/>
                    <b>NeurIPS 2019</b><br/>
                    <a href="https://arxiv.org/abs/1905.12616">pdf</a> |
                    <a href="https://rowanzellers.com/grover/">project page</a> |
                    <a href="http://github.com/rowanz/grover">code</a> |
                    <a href="https://grover.allenai.org/">demo</a> |
                    <a href="https://medium.com/ai2-blog/counteracting-neural-disinformation-with-grover-6cf6690d463b">blog</a>
                </li>
                <li class="publication"><b>Conditional Driving from Natural Language Instructions</b><br/>
                    <i>Junha Roh, Chris Paxton, Andrezej Pronobis, Ali Farhadi, Dieter Fox</i><br/>
                    <b>CoRL 2019</b><br/>
                    <a href="https://arxiv.org/abs/1910.07615">pdf</a> |
                    <a href="https://sites.google.com/view/language-grounded-driving">project page</a> |
                    <a href="https://github.com/rohjunha/language-grounded-driving">code</a>
                </li>
                <li class="publication"><b>Real-Time Open-Domain Question Answering with Dense-Sparse Phrase Index</b><br/>
                    <i>Minjoon Seo, J Lee, Tom Kwiatkowski, AP Parikh, Ali Farhadi, Hannaneh Hajishirzi</i><br/>
                    <b>ACL 2019</b><br/>
                    <a href="https://arxiv.org/abs/1906.05807">pdf</a> |
                    <a href="https://github.com/uwnlp/denspi">code</a>
                </li>
                <li class="publication"><b>HellaSwag: Can a Machine Really Finish Your Sentence?</b><br/>
                    <i>Rowan Zellers, A Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi</i><br/>
                    <b>ACL 2019</b><br/>
                    <a href="https://arxiv.org/abs/1905.07830">pdf</a> |
                    <a href="https://rowanzellers.com/hellaswag/">project page</a> |
                    <a href="https://github.com/rowanz/hellaswag">code</a>
                </li>
                <li class="publication"><b>Learning to Learn How to Learn:Self-Adaptive Visual Navigation Using Meta-Learning</b><br/>
                    <i>Mitchell Wortsman, Kiana Ehsani, Mohammad Rastegari, Ali Farhadi, Roozbeh Mottaghi</i><br/>
                    <b>CVPR 2019</b><br/>
                    <a href="https://arxiv.org/abs/1812.00971">pdf</a> |
                    <a href="https://github.com/allenai/savn">code</a>
                </li>
                <li class="publication"><b>From Recognition to Cognition: Visual Commonsense Reasoning</b><br/>
                    <i>Rowan Zellers, Yonatan Bisk, Ali Farhadi, Yejin Choi</i><br/>
                    <b>CVPR 2019</b><br/>
                    <a href="https://arxiv.org/abs/1811.10830">pdf</a> |
                    <a href="https://visualcommonsense.com/">project page</a> |
                    <a href="https://github.com/rowanz/r2c/">code</a>
                </li>
                <li class="publication"><b>OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge</b><br/>
                    <i>Kenneth Marino, Mohammad Rastegari, Ali Farhadi, Roozbeh Mottaghi</i><br/>
                    <b>CVPR 2019</b><br/>
                    <a href="https://arxiv.org/abs/1906.00067">pdf</a>
                </li>
                <li class="publication"><b>Video Relationship Reasoning using Gated Spatio-Temporal Energy Graph</b><br/>
                    <i>Yao-Hung Hubert Tsai, Santosh Divvala, Louis-Philippe Morency, Ruslan Salakhutdinov, Ali Farhadi</i><br/>
                    <b>CVPR 2019</b><br/>
                    <a href="https://arxiv.org/abs/1903.10547">pdf</a>
                </li>
                <li class="publication"><b>ELASTIC: Improving CNNs with Instance Specific Scaling Policies</b><br/>
                    <i>Huiyu Wang, Aniruddha Kembhavi, Ali Farhadi, Alan Yuille, Mohammad Rastegari</i><br/>
                    <b>CVPR 2019</b><br/>
                    <a href="https://arxiv.org/abs/1812.05262">pdf</a>
                </li>
                <li class="publication"><b>Two Body Problem: Collaborative Visual Task Completion</b><br/>
                    <i>Unnat Jain, Luca Weihs, Eric Kolve, Mohammad Rastegari, Svetlana Lazebnik, Ali Farhadi, Alexander Schwing, Aniruddha Kembhavi</i><br/>
                    <b>CVPR 2019</b><br/>
                    <a href="https://arxiv.org/abs/1904.05879">pdf</a>
                </li>
                <li class="publication"><b>SplitNet: Sim2Sim and Task2Task Transfer for Embodied Visual Navigation</b><br/>
                    <i>Daniel Gordon, Abhishek Kadian, Devi Parikh, Judy Hoffman, Dhruv Batra</i><br/>
                    <b>ICCV 2019</b><br/>
                    <a href="https://danielgordon10.github.io/pdfs/splitnet.pdf">pdf</a> |
                    <a href="https://github.com/facebookresearch/splitnet">code</a>
                </li>
                <li class="publication"><b>Shifting the Baseline: Single Modality Performance on Visual Navigation & QA</b><br/>
                    <i>Jesse Thomason, Daniel Gordon, Yonatan Bisk</i><br/>
                    <b>NAACL 2019 Short Papers</b><br/>
                    <a href="https://danielgordon10.github.io/pdfs/shifting_baseline.pdf">pdf</a>
                </li>
                <li class="publication"><b>Visual Semantic Navigation using Scene Priors</b><br/>
                    <i>Wei Yang, Xiaolong Wang, Ali Farhadi, Abhinav Gupta, Roozbeh Mottaghi</i><br/>
                    <b>ICLR 2019</b><br/>
                    <a href="https://arxiv.org/abs/1810.06543">pdf</a>
                </li>
            </ul>
            <h2>2018</h2>
            <ul>
                <li class="publication"><b>What Should I Do Now? Marrying Reinforcement Learning and Symbolic Planning</b><br/>
                    <i>Daniel Gordon, Dieter Fox, Ali Farhadi</i><br/>
                    <a href="https://danielgordon10.github.io/pdfs/hiprl.pdf">pdf</a> |
                    <a href="https://danielgordon10.github.io/papers/hiprl.html">project page</a> |
                    <a href="https://www.youtube.com/watch?v=0TtWJ_0mPfI">video</a>
                </li>
                <li class="publication"><b>Label refinery: Improving imagenet classification through label progression</b><br/>
                    <i>Hessam Bagherinezhad, Maxwell Horton, Mohammad Rastegari, Ali Farhadi</i><br/>
                    <a href="https://arxiv.org/abs/1805.02641">pdf</a> |
                    <a href="https://github.com/hessamb/label-refinery">code</a>
                </li>
                <li class="publication"><b>YOLOv3: An Incremental Improvement</b><br/>
                    <i>Joseph Redmon, Ali Farhadi</i><br/>
                    <a href="https://arxiv.org/abs/1804.02767">pdf</a> |
                    <a href="https://pjreddie.com/darknet/yolo/">project page</a> |
                    <a href="https://www.youtube.com/watch?v=MPU2HistivI">video</a>
                </li>
                <li class="publication"><b>Phrase-Indexed Question Answering: A New Challenge for Scalable Document Comprehension</b><br/>
                    <i>Minjoon Seo, Tom Kwiatkowski, Ankur P. Parikh, Ali Farhadi, Hannaneh Hajishirzi</i><br/>
                    <b>EMNLP 2018</b><br>
                    <a href="https://arxiv.org/abs/1804.07726">pdf</a> |
                    <a href="https://github.com/uwnlp/piqa">code</a>
                </li>
                <li class="publication"><b>PhotoShape: Photorealistic Materials for Large-Scale Shape Collections</b><br/>
                    <i>Keunhong Park, Konstantinos Rematas, Ali Farhadi, Steve Seitz</i><br/>
                    <b>SIGGRAPH Asia 2018</b><br>
                    <a href="https://arxiv.org/abs/1809.09761">pdf</a> |
                    <a href="https://keunhong.com/publications/photoshape/">project page</a> |
                    <a href="https://github.com/keunhong/photoshape">code</a>
                </li>
                <li class="publication"><b>Imagine This! Scripts to Compositions to Videos</b><br/>
                    <i>Tanmay Gupta, Dustin Schwenk, Ali Farhadi, Derek Hoiem, Aniruddha Kembhavi</i><br/>
                    <b>ECCV 2018</b><br/>
                    <a href="https://arxiv.org/abs/1804.03608">pdf</a>
                </li>
                <li class="publication"><b>Transferring Common-Sense Knowledge for Object Detection</b><br/>
                    <i>Krishna Kumar Singh, Santosh Kumar Divvala, Ali Farhadi, Yong Jae Lee</i><br/>
                    <b>ECCV 2019</b><br/>
                    <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Krishna_Kumar_Singh_Transferring_Common-Sense_Knowledge_ECCV_2018_paper.pdf">pdf</a>
                </li>
                <li class="publication"><b>Who Let The Dogs Out? Modeling Dog Behavior From Visual Data</b><br/>
                    <i>Kiana Ehsani, Hessam Bagherinezhad, Joseph Redmon, Roozbeh Mottaghi, Ali Farhadi</i><br/>
                    <b>CVPR 2018</b><br>
                    <a href="https://arxiv.org/abs/1803.10827">pdf</a> |
                    <a href="https://github.com/ehsanik/dogTorch">code</a>
                </li>
                <li class="publication"><b>Segan: Segmenting and generating the invisible</b><br/>
                    <i>Kiana Ehsani, Roozbeh Mottaghi, Ali Farhadi</i><br/>
                    <b>CVPR 2018</b><br>
                    <a href="https://arxiv.org/abs/1703.10239">pdf</a> |
                    <a href="https://github.com/ehsanik/SeGAN">code</a>
                </li>
                <li class="publication"><b>Actor and Observer: Joint Modeling of First and Third-Person Videos</b><br/>
                    <i>Gunnar Sigurdsson, Abhinav Gupta, Cordelia Schmid, Ali Farhadi, Karteek Alahari</i><br/>
                    <b>CVPR 2018</b><br>
                    <a href="https://arxiv.org/abs/1804.09627">pdf</a>
                </li>
                <li class="publication"><b>Structured Set Matching Networks for One-Shot Part Labeling</b><br/>
                    <i>Jonghyun Choi, Jayant Krishnamurthy, Aniruddha Kembhavi, Ali Farhadi</i><br/>
                    <b>CVPR 2018</b><br>
                    <a href="https://arxiv.org/abs/1712.01867">pdf</a>
                </li>
                <li class="publication"><b>IQA: Visual Question Answering in Interactive Environments</b><br/>
                    <i>Daniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari, Joseph Redmon, Dieter Fox, Ali Farhadi</i><br/>
                    <b>CVPR 2018</b> | <font style="color:green"><b>NVIDIA Pioneering Research Award</b></font><br/>
                    <a href="https://danielgordon10.github.io/pdfs/iqa.pdf">pdf</a> |
                    <a href="https://danielgordon10.github.io/papers/iqa.html">project page</a> |
                    <a href="https://github.com/danielgordon10/thor-iqa-cvpr-2018">code</a> |
                    <a href="https://www.youtube.com/watch?v=pXd3C-1jr98">video</a>
                </li>
                <li class="publication"><b>Neural Speed Reading via Skim-RNN</b><br/>
                    <i>Minjoon Seo, Sewon Min, Ali Farhadi, Hannaneh Hajishirzi</i><br/>
                    <b>ICLR 2018</b><br>
                    <a href="https://arxiv.org/abs/1711.02085">pdf</a>
                </li>
                <li class="publication"><b>Re3: Real-Time Recurrent Regression Networks for Visual Tracking of Generic Objects</b><br/>
                    <i>Daniel Gordon, Ali Farhadi, Dieter Fox</i><br/>
                    <b>RAL 2018</b> | Presented at ICRA 2018<br/>
                    <a href="https://danielgordon10.github.io/pdfs/re3.pdf">pdf</a> |
                    <a href="https://danielgordon10.github.io/papers/re3.html">project page</a> |
                    <a href="https://gitlab.com/danielgordon10/re3-tensorflow">code</a> |
                    <a href="https://www.youtube.com/watch?v=RByCiOLlxug">video</a> |
                    <a href="https://www.youtube.com/watch?v=mRpvhuuwMiY">GTC Talk</a>
                </li>
                <li class="publication"><b>AJILE Movement Prediction: Multimodal Deep Learning for Natural Human Neural Recordings and Video</b><br/>
                    <i>Nancy Xin Ru Wang, Ali Farhadi, Rajesh Rao, Bingni Brunton</i><br/>
                    <b>AAAI 2018</b><br>
                    <a href="https://arxiv.org/abs/1709.05939">pdf</a>
                </li>
            </ul>
            <h2>2017</h2>
            <ul>
                <li class="publication"><b>AI2-THOR: An Interactive 3D Environment for Visual AI</b><br/>
                    <i>Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, Ali Farhadi</i><br/>
                    <a href="https://arxiv.org/abs/1712.05474">pdf</a> |
                    <a href="https://ai2thor.allenai.org/">project page</a> |
                    <a href="https://github.com/allenai/ai2thor">code</a> |
                    <a href="https://www.youtube.com/watch?v=KcELPpdN770">video</a> |
                    <a href="https://ai2thor.allenai.org/demo/">demo</a>
                </li>
                <li class="publication"><b>Visual Semantic Planning using Deep Successor Representations</b><br/>
                    <i>Daniel Gordon*, Yuke Zhu*, Eric Kolve, Dieter Fox, Li Fei-Fei, Abhinav Gupta, Roozbeh Mottaghi, Ali Farhadi</i><br/>
                    <b>ICCV 2017</b><br/>
                    <a href="https://danielgordon10.github.io/pdfs/vsp.pdf">pdf</a> |
                    <a href="https://www.youtube.com/watch?v=_2pYVw6ATKo">video</a>
                </li>
                <li class="publication"><b>See the Glass Half Full: Reasoning about Liquid Containers, their Volume and Content</b><br/>
                    <i>Roozbeh Mottaghi, Connor Schenck, Dieter Fox, Ali Farhadi</i><br/>
                    <b>ICCV 2017</b><br/>
                    <a href="https://arxiv.org/abs/1701.02718">pdf</a>
                </li>
                <li class="publication"><b>YOLO9000: Better, Faster, Stronger</b><br/>
                    <i>Joseph Redmon, Ali Farhadi</i><br/>
                    <b>CVPR 2017</b> | <font style="color:green"><b>Best Paper Honorable Mention</b></font><br/>
                    <a href="https://arxiv.org/abs/1612.08242">pdf</a> |
                    <a href="https://pjreddie.com/darknet/yolo/">project page</a>
                </li>
                <li class="publication"><b>Are You Smarter Than A Sixth Grader? Textbook Question Answering for Multimodal Machine Comprehension</b><br/>
                    <i>Aniruddha Kembhavi, Minjoon Seo, Eric Klove, Dustin Schwenk, Hannaneh Hajishirzi, Ali Farhadi</i><br/>
                    <b>CVPR 2017</b><br/>
                    <a href="http://ai2-website.s3.amazonaws.com/publications/CVPR17_TQA.pdf">pdf</a> |
                    <a href="http://vuchallenge.org/tqa.html">project page</a>
                </li>
                <li class="publication"><b>LCNN: Lookup-based Convolutional Neural Network</b><br/>
                    <i>Hessam Bagherinezhad, Mohammad Rastegari, Ali Farhadi</i><br/>
                    <b>CVPR 2017</b><br/>
                    <a href="https://arxiv.org/abs/1611.06473">pdf</a> |
                    <a href="https://github.com/hessamb/lcnn">code</a>
                </li>
                <li class="publication"><b>Commonly Uncommon: Semantic Sparsity in Situation Recognition</b><br/>
                    <i>Mark Yatskar, Vicente Ordóñez, Luke Zettlemoyer, Ali Farhadi</i><br/>
                    <b>CVPR 2017</b><br/>
                    <a href="https://arxiv.org/abs/1612.00901">pdf</a> |
                    <a href="http://imsitu.org/demo/">demo</a>
                </li>
                <li class="publication"><b>Asynchronous Temporal Fields for Action Recognition</b><br/>
                    <i>Gunnar A Sigurdsson, Santosh Divvala, Ali Farhadi, Abhinav Gupta</i><br/>
                    <b>CVPR 2017</b><br/>
                    <a href="https://arxiv.org/abs/1612.06371">pdf</a>
                </li>
                <li class="publication"><b>Query-Reduction Networks for Question Answering</b><br/>
                    <i>Minjoon Seo, Sewon Min, Ali Farhadi, Hannaneh Hajishirzi</i><br/>
                    <b>ICLR 2017</b><br/>
                    <a href="https://arxiv.org/abs/1606.04582">pdf</a> |
                    <a href="https://github.com/uwnlp/qrn">code</a>
                </li>
                <li class="publication"><b>Bidirectional Attention Flow for Machine Comprehension</b><br/>
                    <i>Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hannaneh Hajishirzi</i><br/>
                    <b>ICLR 2017</b><br/>
                    <a href="https://arxiv.org/abs/1611.01603">pdf</a> |
                    <a href="https://allenai.github.io/bi-att-flow/"> project page</a> |
                    <a href="https://github.com/allenai/bi-att-flow">code</a>
                </li>
                <li class="publication"><b>Target-driven visual navigation in indoor scenes using deep reinforcement learning</b><br/>
                    <i>Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph Lim, Abhinav Gupta, Fei-Fei Li, Ali Farhadi</i><br/>
                    <b>ICRA 2017</b><br/>
                    <a href="https://arxiv.org/abs/1609.05143">pdf</a>
                </li>
                <li class="publication"><b>Summarizing unconstrained videos using salient montages</b><br/>
                    <i>Min Sun, Ali Farhadi, Ben Taskar, Steve Seitz</i><br/>
                    <b>TPAMI 2017</b><br/>
                    <a href="https://ieeexplore.ieee.org/document/7779036">pdf</a>
                </li>
                <li class="publication"><b>Semantic Highlight Retrieval and Term Prediction</b><br/>
                    <i>Min Sun , Kuo-Hao Zeng, Yen-Chen Lin, Ali Farhadi</i><br/>
                    <b>TIP 2017</b><br/>
                    <a href="https://ieeexplore.ieee.org/document/7852483">pdf</a>
                </li>
            </ul>
            <h2>2016</h2>
            <ul>
                <li class="publication"><b>XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks</b><br/>
                    <i>Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, Ali Farhadi</i><br/>
                    <b>ECCV 2016</b><br/>
                    <a href="http://arxiv.org/abs/1603.05279">pdf</a> |
                    <a href="https://github.com/allenai/XNOR-Net">code</a>
                </li>
                <li class="publication"><b>Deep3D: Fully Automatic 2D-to-3D Video Conversion with Deep Convolutional Neural Networks</b><br/>
                    <i>Junyuan Xie, Ross B. Girshick, Ali Farhadi</i><br/>
                    <b>ECCV 2016</b><br/>
                    <a href="https://arxiv.org/abs/1604.03650">pdf</a> |
                    <a href="https://github.com/piiswrong/deep3d">code</a>
                </li>
                <li class="publication"><b>A Diagram Is Worth A Dozen Images</b><br/>
                    <i>Ani Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi</i><br/>
                    <b>ECCV 2016</b><br/>
                    <a href="https://arxiv.org/abs/1603.07396">pdf</a> |
                    <a href="https://github.com/allenai/dqa-net">code</a>
                </li>
                <li class="publication"><b>"What happens if..." Learning to predict the effect of forces in images</b><br/>
                    <i>Roozbeh Mottaghi, Mohammad Rastegari, Abhinav Gupta, Ali Farhadi</i><br/>
                    <b>ECCV 2016</b><br/>
                    <a href="https://arxiv.org/abs/1603.05600">pdf</a>
                </li>
                <li class="publication"><b>Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding</b><br/>
                    <i>Gunnar Sigurdsson, Gul Varol, Xiaolong Wang, Ali Farhadi, Ivan Laptev, Abhinav Gupta</i><br/>
                    <b>ECCV 2016</b><br/>
                    <a href="https://arxiv.org/abs/1604.01753">pdf</a> |
                    <a href="http://allenai.org/plato/charades/">project page</a>
                </li>
                <li class="publication"><b>FigureSeer:Parsing Result-Figures in Research Papers</b><br/>
                    <i>Noah Siegel, Santosh Divvala, Ali Farhadi</i><br/>
                    <b>ECCV 2016</b><br/>
                    <a href="http://ai2-website.s3.amazonaws.com/publications/Siegel16eccv.pdf">pdf</a> |
                    <a href="https://prior.allenai.org/projects/figureseer">project page</a>
                </li>
                <li class="publication"><b>You Only Look Once: Unified, Real-Time Object Detection</b><br/>
                    <i>Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi</i><br/>
                    <b>CVPR 2016</b> | <font style="color:green"><b>OpenCV People's Choice Award</b></font><br/>
                    <a href="http://arxiv.org/abs/1506.02640">pdf</a> |
                    <a href="https://pjreddie.com/darknet/yolo/">project page</a>
                </li>
                <li class="publication"><b>Situation Recognition: Visual Semantic Role Labeling for Image Understanding</b><br/>
                    <i>Mark Yatskar, Luke Zettlemoyer, Ali Farhadi</i><br/>
                    <b>CVPR 2016</b><br/>
                    <a href="https://homes.cs.washington.edu/~ali/papers/SituationRecognition.pdf">pdf</a> |
                    <a href="https://github.com/my89/SituationCrf">code</a> |
                    <a href="https://github.com/my89/imSitu">dataset</a>
                </li>
                <li class="publication"><b>Newtonian Image Understanding: Unfolding the Dynamics of Objects in Static Images</b><br/>
                    <i>Roozbeh Mottaghi, Hessam Bagherinezhad, Mohammad Rastegari, Ali Farhadi</i><br/>
                    <b>CVPR 2016</b><br/>
                    <a href="https://arxiv.org/abs/1511.04048">pdf</a> |
                    <a href="http://allenai.org/plato/newtonian-understanding/">project page</a>
                </li>
                <li class="publication"><b>Actions~Transformation</b><br/>
                    <i>Xiaolong Wang, Ali Farhadi, Abhinav Gupta</i><br/>
                    <b>CVPR 2016</b><br/>
                    <a href="https://arxiv.org/abs/1512.00795">pdf</a>
                </li>
                <li class="publication"><b>A Task-Oriented Approach for Cost-sensitive Recognition</b><br/>
                    <i>Roozbeh Mottaghi, Hannaneh Hajishirzi, Ali Farhadi</i><br/>
                    <b>CVPR 2016</b><br/>
                    <a href="https://homes.cs.washington.edu/~ali/papers/Tasks.pdf">pdf</a>
                </li>
                <li class="publication"><b>Unsupervised Deep Embedding for Clustering Analysis</b><br/>
                    <i>Junyuan Xie, Ross B. Girshick, Ali Farhadi</i><br/>
                    <b>ICML 2016</b><br/>
                    <a href="http://proceedings.mlr.press/v48/xieb16.html">pdf</a> |
                    <a href="https://github.com/piiswrong/dec">code</a>
                </li>
                <li class="publication"><b>Stating the Obvious: Extracting Visual Common Sense Knowledge</b><br/>
                    <i>Mark Yatskar, Vicente Ordóñez, Ali Farhadi</i><br/>
                    <b>NAACL 2016</b><br/>
                    <a href="https://www.aclweb.org/anthology/N16-1023">pdf</a>
                </li>
                <li class="publication"><b>Are Elephants Bigger than Butterflies? Reasoning about Sizes of Objects</b><br/>
                    <i>Hessam Bagherinezhad, Hannaneh Hajishirzi, Yejin Choi, Ali Farhadi</i><br/>
                    <b>AAAI 2016</b><br/>
                    <a href="https://homes.cs.washington.edu/~ali/papers/Abnormality.pdf">pdf</a>
                </li>
                <li class="publication"><b>Toward a Taxonomy and Computational Models of Abnormalities in Images</b><br/>
                    <i>Babak Saleh, Ahmed Elgammal, Jacob Feldman, Ali Farhadi</i><br/>
                    <b>AAAI 2016</b> | <font style="color:green"><b>Best Student Paper Award</b></font><br/>
                    <a href="https://homes.cs.washington.edu/~ali/papers/Tasks.pdf">pdf</a>
                </li>
            </ul>
            <h2>2015</h2>
            <ul>
                <li class="publication"><b>Segment-Phrase Table for Semantic Segmentation, Visual Entailment and Paraphrasing</b><br/>
                    <i>Hamid Izadinia, Fereshteh Sadeghi, Santosh K Divvala, Hannaneh Hajishirzi, Yejin Choi, Ali Farhadi</i><br/>
                    <b>ICCV 2015</b><br/>
                    <a href="https://homes.cs.washington.edu/~ali/spt.pdf">pdf</a>
                </li>
                <li class="publication"><b>Generating Notifications for Missing Actions: Don’t forget to turn the lights off!</b><br/>
                    <i>Bilge Soran, Ali Farhadi, Linda Shapiro</i><br/>
                    <b>ICCV 2015</b><br/>
                    <a href="https://homes.cs.washington.edu/~ali/alarm-iccv.pdf">pdf</a>
                </li>
                <li class="publication"><b>VISALOGY: Answering Visual Analogy Questions</b><br/>
                    <i>Fereshteh Sadeghi, Larry Zittnick, Ali Farhadi</i><br/>
                    <b>NeurIPS 2015</b><br/>
                    <a href="https://papers.nips.cc/paper/5777-visalogy-answering-visual-analogy-questions">pdf</a>
                </li>
                <li class="publication"><b>Real-Time Grasp Detection Using Convolutional Neural Networks</b><br/>
                    <i>Joseph Redmon, Anelia Angelova</i><br/>
                    <b>ICRA 2015</b><br/>
                    <a href="http://arxiv.org/abs/1506.02640">pdf</a>
                </li>
                <li class="publication"><b>Solving Geometry Problems: Combining Text and Diagram Interpretation</b><br/>
                    <i>Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi, Oren Etzioni, Clint Malcolm</i><br/>
                    <b>EMNLP 2015</b><br/>
                    <a href="https://www.aclweb.org/anthology/D15-1171">pdf</a> |
                    <a href="https://geometry.allenai.org/">project page</a>
                </li>
                <li class="publication"><b>VisKE: Visual Knowledge Extraction and Question Answering by Visual Verification of Relation Phrases</b><br/>
                    <i>Fereshteh Sadeghi, Santosh K Divvala, Ali Farhadi</i><br/>
                    <b>CVPR 2015</b><br/>
                    <a href="http://viske.allenai.org/paper/fsadeghi_VisKE.pdf">pdf</a> |
                    <a href="http://viske.allenai.org/">project page</a>
                </li>
                <li class="publication"><b>Discriminative and Consistent Similarities in Instance-Level Multiple Instance Learning</b><br/>
                    <i>Mohammad Rastegari, Hannaneh Hajishirzi, Ali Farhadi</i><br/>
                    <b>CVPR 2015</b><br/>
                    <a href="http://ssli.ee.washington.edu/~hannaneh/papers/MIL.pdf">pdf</a>
                </li>
                <li class="publication"><b>Learning to Select and Order Vacation Photographs</b><br/>
                    <i>Fereshteh Sadeghi, J Rafael Tena, Ali Farhadi, Leonid Sigal</i><br/>
                    <b>WACV 2015</b><br/>
                    <a href="http://homes.cs.washington.edu/~fsadeghi/papers/fsadeghi_album_wacv15.pdf">pdf</a>
                </li>
            </ul>
            <h2>2014</h2>
            <ul>
                <li class="publication"><b>Learning Everything about Anything: Webly-Supervised Visual Concept Learning</b><br/>
                    <i>Santosh K Divvala, Ali Farhadi, Carlos Guestrin</i><br/>
                    <b>CVPR 2014</b><br/>
                    <a href="http://levan.cs.washington.edu/ngrams/objectNgrams_cvpr14.pdf">pdf</a> |
                    <a href="http://levan.cs.uw.edu/">project page</a>
                </li>
                <li class="publication"><b>Incorporating Scene Context and Object Layout into Appearance Modeling</b><br/>
                    <i>Hamid Izadinia, Fereshteh Sadeghi, Ali Farhadi</i><br/>
                    <b>CVPR 2014</b><br/>
                    <a href="http://grail.cs.washington.edu/pub/papers/izadinia2014isc.pdf">pdf</a>
                </li>
                <li class="publication"><b>Failure Prediction in Vision Systems</b><br/>
                    <i>Peng Zhang, Jiuling Wang, Ali Farhadi, Martial Hebert, Devi Parikh</i><br/>
                    <b>CVPR 2014</b><br/>
                    <a href="http://www.ri.cmu.edu/pub_files/2014/3/predicting_failures_of_vision_systems_CVPR2014.pdf">pdf</a>
                </li>
                <li class="publication"><b>Towards Transparent Systems: Semantic Characterization of Failure Modes</b><br/>
                    <i>Aayush Bansal, Ali Farhadi, Devi Parikh</i><br/>
                    <b>ECCV 2014</b><br/>
                    <a href="http://www.cs.cmu.edu/~aayushb/pubs/characterizing_mistakes_eccv2014.pdf">pdf</a>
                </li>
                <li class="publication"><b>Salient montages from unconstrained videos</b><br/>
                    <i>Min Sun, Ali Farhadi, Ben Taskar, Steve Seitz</i><br/>
                    <b>ECCV 2014</b><br/>
                    <a href="http://grail.cs.washington.edu/pub/papers/sun2014smf.pdf">pdf</a>
                </li>
                <li class="publication"><b>Ranking Domain-Specific Highlights by Analyzing Edited Videos</b><br/>
                    <i>Min Sun, Ali Farhadi, Steve Seitz</i><br/>
                    <b>ECCV 2014</b><br/>
                    <a href="http://grail.cs.washington.edu/pub/papers/sun2014rdh.pdf">pdf</a>
                </li>
                <li class="publication"><b>Diagram Understanding in Geometry Questions</b><br/>
                    <i>Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi, Oren Etzioni</i><br/>
                    <b>AAAI 2014</b><br/>
                    <a href="http://homes.cs.washington.edu/~minjoon/papers/geosolver/diagram_understanding.pdf">pdf</a> |
                    <a href="https://geometry.allenai.org/">project page</a>
                </li>
                <li class="publication"><b>Multi Resolution Language Grounding with Weak Supervision</b><br/>
                    <i>Rik Koncel Kedziorski, Hannaneh Hajishirzi, and Ali Farhadi</i><br/>
                    <b>EMNLP 2014</b><br/>
                    <a href="http://ssli.ee.washington.edu/~hannaneh/segmentation-emnlp14.pdf">pdf</a>
                </li>
                <li class="publication"><b>Action Recognition in the Presence of One Egocentric and Multiple Static Cameras</b><br/>
                    <i>Bilge Soran, Ali Farhadi, Linda Shapiro</i><br/>
                    <b>ACCV 2014</b><br/>
                    <a href="http://homes.cs.washington.edu/~shapiro/accv2014finalpaper.pdf">pdf</a>
                </li>
            </ul>
            <h2>2013</h2>
            <ul>
                <li class="publication"><b>Multi-Attribute Queries: To Merge or Not to Merge?</b><br/>
                    <i>Mohammad Rastegari, Ali Diba, Devi Parikh, Ali Farhadi</i><br/>
                    <b>CVPR 2013</b><br/>
                    <a href="https://homes.cs.washington.edu/~ali/papers/MultiAtr_CVPR13.pdf">pdf</a>
                </li>
                <li class="publication"><b>Object-Centric Anomaly Detection by Attribute-Based Reasoning</b><br/>
                    <i>Babak Saleh, Ali Farhadi, Ahmed Elgammal</i><br/>
                    <b>CVPR 2013</b><br/>
                    <a href="https://homes.cs.washington.edu/~ali/papers/Abnormality_CVPR13.pdf">pdf</a>
                </li>
                <li class="publication"><b>Adding Unlabeled Samples to Categories by Learned Attributes</b><br/>
                    <i>Jonghyun Choi, Mohammad Rastegari, Ali Farhadi, Larry Davis</i><br/>
                    <b>CVPR 2013</b><br/>
                    <a href="https://homes.cs.washington.edu/~ali/papers/Adding_CVPR13.pdf">pdf</a>
                </li>
            </ul>
            <h2>2012</h2>
            <ul>
                <li class="publication"><b>Attribute Discovery via Predictable Discriminative Binary Codes</b><br/>
                    <i>Mohammad Rastegari, Ali Farhadi, David Forsyth</i><br/>
                    <b>ECCV 2012</b><br/>
                    <a href="http://www.cs.umd.edu/~mrastega/Site/Publications_files/dbc.pdf">pdf</a>
                </li>
                <li class="publication"><b>Semantic Understanding of Proefessional Soccer Commentaries</b><br/>
                    <i>Hannaneh Hajishirzi, Mohammad Rastegari, Ali Farhadi, Jessica Hodgins</i><br/>
                    <b>UAI 2012</b><br/>
                    <a href="http://homes.cs.washington.edu/~hannaneh/soccer-UAI.pdf">pdf</a>
                </li>
                <li class="publication"><b>Building a Dictionary of Image Fragments</b><br/>
                    <i>Zicheng Liao, Ali Farhadi, Yang Wang, Ian Endres, David Forsyth</i><br/>
                    <b>CVPR 2012</b><br/>
                    <a href="http://web.engr.illinois.edu/~liao17/data/fragdict-cvpr12.pdf">pdf</a>
                </li>
            </ul>
            <h2>2011</h2>
            <ul>
                <li class="publication"><b>Understanding Egocentric Activities</b><br/>
                    <i>Alireza Fathi, Ali Farhadi, James Rehg</i><br/>
                    <b>ICCV 2011</b><br/>
                    <a href="http://www.cc.gatech.edu/~afathi3/publication/ICCV11.pdf">pdf</a>
                </li>
                <li class="publication"><b>Recognition Using Visual Phrases</b><br/>
                    <i>Ali Farhadi, Amin Sadeghi</i><br/>
                    <b>CVPR 2011</b> | <font style="color:green"><b>Best Student Paper Award</b></font><br/>
                    <a href="http://vision.cs.uiuc.edu/phrasal/recognition_using_visual_phrases.pdf">pdf</a>
                </li>
                <li class="publication"><b>Using Classification to Protect the Integrity of Spectrum Measurements in White Space Networks</b><br/>
                    <i>Omid Fatemieh, Ali Farhadi, Ranveer Chandra, Carl Gunter</i><br/>
                    <b>NDSS 2011</b><br/>
                    <a href="http://seclab.uiuc.edu/pubs/FatemiehFCG11.pdf">pdf</a>
                </li>
            </ul>
            <h2>2010</h2>
            <ul>
                <li class="publication"><b>Every Picture Tells a Story: Generating Sentences for Images</b><br/>
                    <i>Ali Farhadi, Mohsen Hejrati, Amin Sadeghi, Peter Young, Cyrus Rashtchian, Julia Hockenmaier, David Forsyth</i><br/>
                    <b>ECCV 2010</b><br/>
                    <a href="https://homes.cs.washington.edu/~ali/papers/sentence.pdf">pdf</a>
                </li>
                <li class="publication"><b>Attribute-Centric Recognition for Cross-Category Generalization</b><br/>
                    <i>Ali Farhadi, Ian Endres, Derek Hoiem</i><br/>
                    <b>CVPR 2010</b><br/>
                    <a href="https://homes.cs.washington.edu/~ali/papers/attributes_2010.pdf">pdf</a>
                </li>
            </ul>
            <h2>2009</h2>
            <ul>
                <li class="publication"><b>A Latent Model of Discriminative Aspect</b><br/>
                    <i>Ali Farhadi, Mostafa Kamali, Ian Endres, David Forsyth</i><br/>
                    <b>ICCV 2009</b><br/>
                    <a href="https://homes.cs.washington.edu/~ali/papers/ICCV09_Aspect.pdf">pdf</a>
                </li>
                <li class="publication"><b>Unlabeled Data Improves Word Prediction</b><br/>
                    <i>Nicolas Loeff, Ali Farhadi, Ian Endres, David Forsyth</i><br/>
                    <b>ICCV 2009</b><br/>
                    <a href="https://homes.cs.washington.edu/~ali/papers/ICCV09_manifold.pdf">pdf</a>
                </li>
                <li class="publication"><b>Describing Objects by their Attributes</b><br/>
                    <i>Ali Farhadi, Ian Endres, Derek Hoiem, David Forsyth</i><br/>
                    <b>CVPR 2009</b><br/>
                    <a href="https://homes.cs.washington.edu/~ali/papers/Attributes.pdf">pdf</a>
                </li>
            </ul>
            <h2>2006-2008</h2>
            <ul>
                <li class="publication"><b>Learning to Recognize Activities from a Wrong Viewpoint</b><br/>
                    <i>Ali Farhadi, Mostafa Kamali</i><br/>
                    <b>ECCV 2008</b><br/>
                    <a href="https://homes.cs.washington.edu/~ali/papers/Activity_transfer.pdf">pdf</a>
                </li>
                <li class="publication"><b>Scene Discovery by Matrix Factorization</b><br/>
                    <i>Nicolas Loeff, Ali Farhadi</i><br/>
                    <b>ECCV 2008</b><br/>
                    <a href="https://homes.cs.washington.edu/~ali/papers/scene_discovery.pdf">pdf</a>
                </li>
                <li class="publication"><b>Transfer Learning in Sign Language</b><br/>
                    <i>Ali Farhadi, David Forsyth, Ryan White</i><br/>
                    <b>CVPR 2007</b><br/>
                    <a href="https://homes.cs.washington.edu/~ali/papers/Transfer_Learning_ASL.pdf">pdf</a>
                </li>
                <li class="publication"><b>Aligning ASL for Statistical Translation Using a Discriminative Word Model</b><br/>
                    <i>Ali Farhadi, David Forsyth</i><br/>
                    <b>CVPR 2006</b><br/>
                    <a href="https://homes.cs.washington.edu/~ali/papers/ASL_CVPR06.pdf">pdf</a>
                </li>
            </ul>
        </div>
    </div>
</div>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
<script src="./js/bootstrap.min.js"></script>


</body>

</html>
